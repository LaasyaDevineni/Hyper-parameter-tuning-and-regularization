# Hyperparameter Tuning with Keras on MNIST

## Overview

This project demonstrates the impact of hyperparameter tuning on the performance of a multi-layer perceptron (MLP) model using the MNIST dataset. Various techniques are applied, including weight initialization, activation functions, optimizers, batch normalization, and dropout regularization, to enhance the model's accuracy.

## Objectives

- To explore and implement various hyperparameter tuning techniques.
- To analyze the performance improvements on the MLP model.
- To achieve optimal accuracy on the MNIST dataset.

## Dataset

The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9). The dataset is split into 60,000 training images and 10,000 test images.

## Installation

To run this project, ensure you have the following dependencies installed:

```bash
pip install numpy matplotlib keras

